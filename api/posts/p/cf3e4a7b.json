{"data":{"title":"Scrapy-入门篇","slug":"技术学习/Scrapy-入门篇","description":"创建第一个 Scrapy 爬虫","date":"2024-02-19T16:54:08.000Z","updated":"2025-07-30T15:34:40.523Z","language":"zh-CN","comments":true,"url":"p/cf3e4a7b/","cover":"https://cdn.gallery.uuanqin.top/img/20240220045858.webp","images":[],"content":"<p>Scrapy (/ˈskreɪpaɪ/) 是一个爬取网页并提取结构化数据的应用框架，使用 Python 编写。官方文档：<a href=\"https://docs.scrapy.org/en/latest/intro/overview.html\">Scrapy at a glance — Scrapy 2.11.1 documentation</a>。网上有许多教程，爬虫做是做出来了，但其中的原理可能讲得还不够清晰。我认为阅读官方文档将更有助于理解。</p>\n<p>文章内容主要基于官方文档第一章 FIRST STEPS 进行补充。本文将通过爬取英文名人名言网站 <a href=\"https://quotes.toscrape.com\">https://quotes.toscrape.com</a> 的例子初步理解 Scrapy，学会自己编写一个简单的爬虫。</p>\n<blockquote>\n<p>框架：是一个半成品软件，是一套可重用的、通用的、软件基础代码模型。基于框架进行开发，更加快捷、更加高效。</p>\n</blockquote>\n<h1 id=\"安装\"><a class=\"markdownIt-Anchor\" href=\"#安装\"></a> 安装</h1>\n<p>可以使用 pip 进行安装：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install Scrapy</span><br></pre></td></tr></table></figure>\n<p>推荐单独为 Scrapy 创建一个 Python 虚拟环境（venv），不管是在哪个平台，以避免和系统中的其它包产生冲突。</p>\n<h1 id=\"创建-scrapy-项目\"><a class=\"markdownIt-Anchor\" href=\"#创建-scrapy-项目\"></a> 创建 Scrapy 项目</h1>\n<p>在你想要创建 Scrapy 项目的位置执行以下命令：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scrapy startproject tutorial <span class=\"comment\"># tutorial 改为你想要的项目名字</span></span><br></pre></td></tr></table></figure>\n<p>成功效果如图：</p>\n<p><img src= \"/image/loading.gif\" data-lazy-src=\"https://cdn.gallery.uuanqin.top/img/20240220012641.webp\" alt=\"image.png\" /></p>\n<p>这时将会多出一个名为 tutorial，其内容如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.</span><br><span class=\"line\">│   scrapy.cfg         # 部署参数文件</span><br><span class=\"line\">└───tutorial           # 项目的Python模块，你将从这里导入你的代码</span><br><span class=\"line\">    │   items.py       # 项目中定义 items 的文件</span><br><span class=\"line\">    │   middlewares.py # 项目middlewares文件</span><br><span class=\"line\">    │   pipelines.py   # 项目pipelines文件</span><br><span class=\"line\">    │   settings.py    # 项目设置文件</span><br><span class=\"line\">    │   __init__.py</span><br><span class=\"line\">    └───spiders        # 存放爬虫的文件夹 </span><br><span class=\"line\">            __init__.py </span><br></pre></td></tr></table></figure>\n<p>我们需要定义一个类（这个类必须是 <code>scrapy.Spider</code> 的子类），Scrapy 使用这个类的信息去爬取网站。这个类定义了：</p>\n<ul>\n<li>爬取的第一个网站</li>\n<li>【可选】如何继续生成下一个需要爬取的网页</li>\n<li>【可选】如何解析下载下来的页面并提取信息</li>\n</ul>\n<p>我们可以在文件夹 <code>tutorial/spiders</code> 下创建我们第一个爬虫，文件名为 <code>quotes_spider.py</code>：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> pathlib <span class=\"keyword\">import</span> Path</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> scrapy</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">QuotesSpider</span>(scrapy.Spider):</span><br><span class=\"line\">    name = <span class=\"string\">&quot;quotes&quot;</span> <span class=\"comment\"># 定义标识这个爬虫的名字，必须在整个项目中唯一</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">start_requests</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">\t    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">\t    定义爬虫从哪里开始爬取。</span></span><br><span class=\"line\"><span class=\"string\">\t    必须返回一个Requests类的可迭代对象（即一个request的列表或者写一个生成器函数，本示例为后者）。后续的请求将从这些初始请求依次生成。</span></span><br><span class=\"line\"><span class=\"string\">\t    &quot;&quot;&quot;</span></span><br><span class=\"line\">        urls = [</span><br><span class=\"line\">            <span class=\"string\">&quot;https://quotes.toscrape.com/page/1/&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;https://quotes.toscrape.com/page/2/&quot;</span>,</span><br><span class=\"line\">        ]</span><br><span class=\"line\">        <span class=\"keyword\">for</span> url <span class=\"keyword\">in</span> urls:</span><br><span class=\"line\">            <span class=\"keyword\">yield</span> scrapy.Request(url=url, callback=<span class=\"variable language_\">self</span>.parse)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">parse</span>(<span class=\"params\">self, response</span>):</span><br><span class=\"line\">\t    <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">\t    用于处理每一个request对应的response的方法。</span></span><br><span class=\"line\"><span class=\"string\">\t    参数response是TextResponse的一个实例，它包含下载下来的页面的内容以及有用的处理函数。</span></span><br><span class=\"line\"><span class=\"string\">\t    parse()通常用于解析response，提取其中的数据形成字典（dict），然后提取下一个将要被爬取的URL链接并从中创建新的Request请求。</span></span><br><span class=\"line\"><span class=\"string\">\t    &quot;&quot;&quot;</span></span><br><span class=\"line\">        page = response.url.split(<span class=\"string\">&quot;/&quot;</span>)[-<span class=\"number\">2</span>]</span><br><span class=\"line\">        filename = <span class=\"string\">f&quot;quotes-<span class=\"subst\">&#123;page&#125;</span>.html&quot;</span></span><br><span class=\"line\">        Path(filename).write_bytes(response.body)</span><br><span class=\"line\">        <span class=\"variable language_\">self</span>.log(<span class=\"string\">f&quot;Saved file <span class=\"subst\">&#123;filename&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>\n<p>回到项目的顶层目录，执行以下命令运行我们刚刚创建的爬虫：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scrapy crawl quotes</span><br></pre></td></tr></table></figure>\n<p><img src= \"/image/loading.gif\" data-lazy-src=\"https://cdn.gallery.uuanqin.top/img/20240220015148.webp\" alt=\"image.png\" /></p>\n<p>项目下多出以下两个文件，这两个文件就是两个网站的 HTML 数据。我们将在下一小节实现对数据的提取。</p>\n<p><img src= \"/image/loading.gif\" data-lazy-src=\"https://cdn.gallery.uuanqin.top/img/20240220015246.webp\" alt=\"image.png\" /></p>\n<p>让我们看看执行这个命令发生了什么？</p>\n<ol>\n<li><code>start_requests()</code> 方法返回 <code>scrapy.Request</code> 类供 Scrapy 进行调度</li>\n<li>收到每一个 Request 的回复（response）后，Scrapy 实例化 <code>Response</code>，并调用对应 Request 的回调函数（在这个例子中回调函数为 <code>parse()</code>），将 <code>Response</code> 作为回调函数中的参数。</li>\n</ol>\n<p>上面的代码我们还可以进行简化：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> pathlib <span class=\"keyword\">import</span> Path</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> scrapy</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">QuotesSpider</span>(scrapy.Spider):</span><br><span class=\"line\">    name = <span class=\"string\">&quot;quotes&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># 我们可以直接定义全局变量start_urls，不用编写start_requests()函数</span></span><br><span class=\"line\">    start_urls = [</span><br><span class=\"line\">        <span class=\"string\">&quot;https://quotes.toscrape.com/page/1/&quot;</span>,</span><br><span class=\"line\">        <span class=\"string\">&quot;https://quotes.toscrape.com/page/2/&quot;</span>,</span><br><span class=\"line\">    ]</span><br><span class=\"line\">\t<span class=\"comment\"># 虽然这里我们没有指明每一个request产生的response如何处理，但Scrapy默认parse()就是它们的回调函数。</span></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">parse</span>(<span class=\"params\">self, response</span>):</span><br><span class=\"line\">        page = response.url.split(<span class=\"string\">&quot;/&quot;</span>)[-<span class=\"number\">2</span>]</span><br><span class=\"line\">        filename = <span class=\"string\">f&quot;quotes-<span class=\"subst\">&#123;page&#125;</span>.html&quot;</span></span><br><span class=\"line\">        Path(filename).write_bytes(response.body)</span><br></pre></td></tr></table></figure>\n<h1 id=\"提取数据的方法\"><a class=\"markdownIt-Anchor\" href=\"#提取数据的方法\"></a> 提取数据的方法</h1>\n<blockquote>\n<p>如果你掌握浏览器开发者工具的使用，我们可以很轻松的完成这件事情（详看 <a class=\"uuanqin-bilink\" target=\"_blank\" href=\"/p/dfbe1dad/\"><span class=\"yukari\">站内文章</span>Scrapy-充分利用浏览器中的开发者工具</a>）。</p>\n</blockquote>\n<h2 id=\"使用-css-选择器\"><a class=\"markdownIt-Anchor\" href=\"#使用-css-选择器\"></a> 使用 CSS 选择器</h2>\n<p>按照官方文档中的教程，我们可以使用一个工具 Scrapy shell 对爬取的页面进行分析从而选择我们需要提取的数据。</p>\n<p>在终端执行命令，我们会进入一个交互程序：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scrapy shell <span class=\"string\">&#x27;https://quotes.toscrape.com/page/1/&#x27;</span> <span class=\"comment\"># windows中可以使用双引号</span></span><br><span class=\"line\"><span class=\"comment\"># 注意链接使用引号括住，否则在链接包含参数（即链接中存在符号`&amp;`）的情况下命令会不起作用。</span></span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>输入 <code>quit()</code> 可退出 Scrapy shell</p>\n</blockquote>\n<p>我们可以 CSS 选择器选择 response 的元素。比如：<code>response.css(&quot;title&quot;)</code></p>\n<p><img src= \"/image/loading.gif\" data-lazy-src=\"https://cdn.gallery.uuanqin.top/img/20240220021614.webp\" alt=\"image.png\" /></p>\n<p>上图显示，CSS 选择器查询结果为 <code>Selector</code> 列表，<code>SelectorList</code>。这些选择器允许你进行更深入地查询或提取数据。</p>\n<p>你可以继续进行交互：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">1</span>]: response.css(<span class=\"string\">&quot;title&quot;</span>)</span><br><span class=\"line\">Out[<span class=\"number\">1</span>]: [&lt;Selector xpath=<span class=\"string\">&#x27;descendant-or-self::title&#x27;</span> data=<span class=\"string\">&#x27;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#x27;</span>&gt;]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加上::text表明我们只选择标签内的文本。getall()方法将返回一个列表，包含所有可能的结果。</span></span><br><span class=\"line\">In [<span class=\"number\">2</span>]: response.css(<span class=\"string\">&quot;title::text&quot;</span>).getall()</span><br><span class=\"line\">Out[<span class=\"number\">2</span>]: [<span class=\"string\">&#x27;Quotes to Scrape&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 不加上的话我们将得到包含标签在内的内容</span></span><br><span class=\"line\">In [<span class=\"number\">3</span>]: response.css(<span class=\"string\">&quot;title&quot;</span>).getall()</span><br><span class=\"line\">Out[<span class=\"number\">3</span>]: [<span class=\"string\">&#x27;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># get() 直接返回第一个结果。若选择器无结果返回None。</span></span><br><span class=\"line\">In [<span class=\"number\">4</span>]: response.css(<span class=\"string\">&quot;title::text&quot;</span>).get()</span><br><span class=\"line\">Out[<span class=\"number\">4</span>]: <span class=\"string\">&#x27;Quotes to Scrape&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 除此之外还可以这样写。但是如果css没有结果将报错 IndexError: list index out of range</span></span><br><span class=\"line\">In [<span class=\"number\">5</span>]: response.css(<span class=\"string\">&quot;title::text&quot;</span>)[<span class=\"number\">0</span>].get()</span><br><span class=\"line\">Out[<span class=\"number\">5</span>]: <span class=\"string\">&#x27;Quotes to Scrape&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 我们还可以使用正则表达式</span></span><br><span class=\"line\">In [<span class=\"number\">6</span>]: response.css(<span class=\"string\">&quot;title::text&quot;</span>).re(<span class=\"string\">r&quot;(\\w+) to (\\w+)&quot;</span>)</span><br><span class=\"line\">Out[<span class=\"number\">6</span>]: [<span class=\"string\">&#x27;Quotes&#x27;</span>, <span class=\"string\">&#x27;Scrape&#x27;</span>]</span><br></pre></td></tr></table></figure>\n<p>如果你想直接打开爬取到的页面，输入以下命令后页面将从浏览器中被打开方便你使用浏览器自带的调试工具找出正确的 CSS 选择器。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">7</span>]: view(response)</span><br><span class=\"line\">Out[<span class=\"number\">7</span>]: <span class=\"literal\">True</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"使用-xpath\"><a class=\"markdownIt-Anchor\" href=\"#使用-xpath\"></a> 使用 XPath</h2>\n<p>Scrapy 支持 XPath 表达式。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">In [<span class=\"number\">8</span>]: response.xpath(<span class=\"string\">&quot;//title&quot;</span>)</span><br><span class=\"line\">Out[<span class=\"number\">8</span>]: [&lt;Selector xpath=<span class=\"string\">&#x27;//title&#x27;</span> data=<span class=\"string\">&#x27;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#x27;</span>&gt;]</span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">9</span>]: response.xpath(<span class=\"string\">&quot;//title/text()&quot;</span>).get()</span><br><span class=\"line\">Out[<span class=\"number\">9</span>]: <span class=\"string\">&#x27;Quotes to Scrape&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>在 Scrapy 中，推荐使用 XPath 表达。</p>\n<blockquote>\n<p>实际上，Scrapy 处理 CSS 选择器最终还是会转换成 XPath 表达。你可以在 shell 中发现这个过程。</p>\n</blockquote>\n<h2 id=\"关于提取的方法\"><a class=\"markdownIt-Anchor\" href=\"#关于提取的方法\"></a> 关于提取的方法</h2>\n<p>不管是 css 选择器还是 XPath 表达，都可以使用以下方法提取我们需要的内容：</p>\n<table>\n<thead>\n<tr>\n<th>方法</th>\n<th>描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>extract()</code></td>\n<td>返回的是符合要求的所有的数据，存在一个列表里。</td>\n</tr>\n<tr>\n<td><code>extract_first()</code></td>\n<td>返回的 hrefs 列表里的第一个数据。</td>\n</tr>\n<tr>\n<td><code>get()</code></td>\n<td>和 extract_first() 方法返回的是一样的，都是列表里的第一个数据。</td>\n</tr>\n<tr>\n<td><code>getall()</code></td>\n<td>和 extract() 方法一样，返回的都是符合要求的所有的数据，存在一个列表里。</td>\n</tr>\n</tbody>\n</table>\n<p>注意：</p>\n<ul>\n<li>get() 、getall() 方法是新的方法，取不到就 raise 一个错误。</li>\n<li>extract() 、extract_first() 方法是旧的方法，取不到就返回 None。</li>\n</ul>\n<blockquote>\n<p>本文按照官方文档示例继续使用旧的 get、getall 方法</p>\n</blockquote>\n<h1 id=\"爬虫中编写提取数据的逻辑\"><a class=\"markdownIt-Anchor\" href=\"#爬虫中编写提取数据的逻辑\"></a> 爬虫中编写提取数据的逻辑</h1>\n<p>在目标网站 <a href=\"https://quotes.toscrape.com\">https://quotes.toscrape.com</a> 中，每一句名人名言的 HTML 基本结构如下（以某一句名言为例）：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;quote&quot;</span> <span class=\"attr\">itemscope</span>=<span class=\"string\">&quot;&quot;</span> <span class=\"attr\">itemtype</span>=<span class=\"string\">&quot;http://schema.org/CreativeWork&quot;</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">span</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;text&quot;</span> <span class=\"attr\">itemprop</span>=<span class=\"string\">&quot;text&quot;</span>&gt;</span>“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”<span class=\"tag\">&lt;/<span class=\"name\">span</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">span</span>&gt;</span>by <span class=\"tag\">&lt;<span class=\"name\">small</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;author&quot;</span> <span class=\"attr\">itemprop</span>=<span class=\"string\">&quot;author&quot;</span>&gt;</span>Albert Einstein<span class=\"tag\">&lt;/<span class=\"name\">small</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/author/Albert-Einstein&quot;</span>&gt;</span>(about)<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;/<span class=\"name\">span</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">div</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tags&quot;</span>&gt;</span></span><br><span class=\"line\">\t\tTags:</span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">meta</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;keywords&quot;</span> <span class=\"attr\">itemprop</span>=<span class=\"string\">&quot;keywords&quot;</span> <span class=\"attr\">content</span>=<span class=\"string\">&quot;change,deep-thoughts,thinking,world&quot;</span>&gt;</span> </span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tag/change/page/1/&quot;</span>&gt;</span>change<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tag/deep-thoughts/page/1/&quot;</span>&gt;</span>deep-thoughts<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tag/thinking/page/1/&quot;</span>&gt;</span>thinking<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;tag&quot;</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/tag/world/page/1/&quot;</span>&gt;</span>world<span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">div</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>我们可以在 Scrapy shell 中一步步找出我们想要的数据：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scrapy shell <span class=\"string\">&#x27;https://quotes.toscrape.com&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">In [<span class=\"number\">1</span>]: response.css(<span class=\"string\">&quot;div.quote&quot;</span>)</span><br><span class=\"line\">Out[<span class=\"number\">1</span>]:</span><br><span class=\"line\">[&lt;Selector xpath=<span class=\"string\">&quot;descendant-or-self::div[@class and contains(concat(&#x27; &#x27;, normalize-space(@class), &#x27; &#x27;), &#x27; quote &#x27;)]&quot;</span> data=<span class=\"string\">&#x27;&lt;div class=&quot;quote&quot; itemscope itemtype...&#x27;</span>&gt;,</span><br><span class=\"line\">......</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定位名言</span></span><br><span class=\"line\">In [<span class=\"number\">2</span>]: quote = response.css(<span class=\"string\">&quot;div.quote&quot;</span>)[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定位并验证名言文本</span></span><br><span class=\"line\">In [<span class=\"number\">3</span>]: text = quote.css(<span class=\"string\">&quot;span.text::text&quot;</span>).get()</span><br><span class=\"line\">In [<span class=\"number\">4</span>]: text</span><br><span class=\"line\">Out[<span class=\"number\">4</span>]: <span class=\"string\">&#x27;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定位并验证名言作者</span></span><br><span class=\"line\">In [<span class=\"number\">5</span>]: author = quote.css(<span class=\"string\">&quot;small.author::text&quot;</span>).get()</span><br><span class=\"line\">In [<span class=\"number\">6</span>]: author</span><br><span class=\"line\">Out[<span class=\"number\">6</span>]: <span class=\"string\">&#x27;Albert Einstein&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定位并验证名言标签</span></span><br><span class=\"line\">In [<span class=\"number\">7</span>]: tags = quote.css(<span class=\"string\">&quot;div.tags a.tag::text&quot;</span>).getall()</span><br><span class=\"line\">In [<span class=\"number\">8</span>]: tags</span><br><span class=\"line\">Out[<span class=\"number\">8</span>]: [<span class=\"string\">&#x27;change&#x27;</span>, <span class=\"string\">&#x27;deep-thoughts&#x27;</span>, <span class=\"string\">&#x27;thinking&#x27;</span>, <span class=\"string\">&#x27;world&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 尝试编写一段这样的逻辑，验证一下我们上面定位是否都准确</span></span><br><span class=\"line\">In [<span class=\"number\">13</span>]: <span class=\"keyword\">for</span> quote <span class=\"keyword\">in</span> response.css(<span class=\"string\">&quot;div.quote&quot;</span>):</span><br><span class=\"line\">    ...:     text = quote.css(<span class=\"string\">&quot;span.text::text&quot;</span>).get()</span><br><span class=\"line\">    ...:     author = quote.css(<span class=\"string\">&quot;small.author::text&quot;</span>).get()</span><br><span class=\"line\">    ...:     tags = quote.css(<span class=\"string\">&quot;div.tags a.tag::text&quot;</span>).getall()</span><br><span class=\"line\">    ...:     <span class=\"built_in\">print</span>(<span class=\"built_in\">dict</span>(text=text, author=author, tags=tags))</span><br><span class=\"line\">    ...:</span><br><span class=\"line\">&#123;<span class=\"string\">&#x27;text&#x27;</span>: <span class=\"string\">&#x27;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&#x27;</span>, <span class=\"string\">&#x27;author&#x27;</span>: <span class=\"string\">&#x27;Albert Einstein&#x27;</span>, <span class=\"string\">&#x27;tags&#x27;</span>: [<span class=\"string\">&#x27;change&#x27;</span>, <span class=\"string\">&#x27;deep-thoughts&#x27;</span>, <span class=\"string\">&#x27;thinking&#x27;</span>, <span class=\"string\">&#x27;world&#x27;</span>]&#125;</span><br><span class=\"line\">&#123;<span class=\"string\">&#x27;text&#x27;</span>: <span class=\"string\">&#x27;“It is our choices, Harry, that show what we truly are, far more than our abilities.”&#x27;</span>, <span class=\"string\">&#x27;author&#x27;</span>: <span class=\"string\">&#x27;J.K. Rowling&#x27;</span>, <span class=\"string\">&#x27;tags&#x27;</span>: [<span class=\"string\">&#x27;abilities&#x27;</span>, <span class=\"string\">&#x27;choices&#x27;</span>]&#125;</span><br><span class=\"line\">......</span><br></pre></td></tr></table></figure>\n<p>验证无误后，我们将这段代码写入爬虫的 parse() 中：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">parse</span>(<span class=\"params\">self, response</span>):</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> quote <span class=\"keyword\">in</span> response.css(<span class=\"string\">&quot;div.quote&quot;</span>):</span><br><span class=\"line\">\t\t<span class=\"keyword\">yield</span> &#123;</span><br><span class=\"line\">\t\t\t<span class=\"string\">&quot;text&quot;</span>: quote.css(<span class=\"string\">&quot;span.text::text&quot;</span>).get(),</span><br><span class=\"line\">\t\t\t<span class=\"string\">&quot;author&quot;</span>: quote.css(<span class=\"string\">&quot;small.author::text&quot;</span>).get(),</span><br><span class=\"line\">\t\t\t<span class=\"string\">&quot;tags&quot;</span>: quote.css(<span class=\"string\">&quot;div.tags a.tag::text&quot;</span>).getall(),</span><br><span class=\"line\">\t\t&#125;</span><br></pre></td></tr></table></figure>\n<p>终端运行以下命令：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scrapy crawl quotes <span class=\"comment\"># 还记得吗？quotes是我们编写的爬虫的名字</span></span><br></pre></td></tr></table></figure>\n<p>你可以在终端输出的日志中读到爬取的内容：</p>\n<p><img src= \"/image/loading.gif\" data-lazy-src=\"https://cdn.gallery.uuanqin.top/img/20240220033017.webp\" alt=\"image.png\" /></p>\n<h1 id=\"存储抓取的数据\"><a class=\"markdownIt-Anchor\" href=\"#存储抓取的数据\"></a> 存储抓取的数据</h1>\n<p>最简单的方式为 Feed exports：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scrapy crawl quotes -O quotes.json</span><br><span class=\"line\"><span class=\"comment\"># quotes 为爬虫的名字</span></span><br><span class=\"line\"><span class=\"comment\"># -O （大写字母O) 表示将覆写已经存在的同名文件；-o 则是将内容附加在已经存在的文件</span></span><br><span class=\"line\"><span class=\"comment\"># quotes.json为输出的文件名</span></span><br></pre></td></tr></table></figure>\n<p>我们可以指定输出文件的格式。上例为 JSON，但是如果我们追加输出到已经存在的文件时会导致生成无效的 json 文件。为此我们可以使用 JSON Lines 格式：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scrapy crawl quotes -o quotes.jsonlines <span class=\"comment\"># 亲测jsonl会报错</span></span><br></pre></td></tr></table></figure>\n<p><img src= \"/image/loading.gif\" data-lazy-src=\"https://cdn.gallery.uuanqin.top/img/20240220033829.webp\" alt=\"image.png\" /></p>\n\n<div class=\"callout\" data-callout=\"hint\">\n<div class=\"callout-title\">\n<div class=\"callout-title-icon\">\n<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"lucide lucide-flame\"><path d=\"M8.5 14.5A2.5 2.5 0 0 0 11 12c0-1.38-.5-2-1-3-1.072-2.143-.224-4.054 2-6 .5 2.5 2 4.9 4 6.5 2 1.6 3 3.5 3 5.5a7 7 0 1 1-14 0c0-1.153.433-2.294 1-3a2.5 2.5 0 0 0 2.5 2.5z\"/></svg>\n</div>\n<div class=\"callout-title-inner\">使用 Scrapy 爬取中文数据时的注意事项</div>\n</div>\n<div class=\"callout-content\"><p>如果爬取数据为中文，存储到文件可能是一堆 unicode 编码。我们可以在 <code>setting.py</code> 中增加：<code>FEED_EXPORT_ENCODING = 'utf-8'</code> 即可</p>\n</div></div><h1 id=\"递归翻页爬取\"><a class=\"markdownIt-Anchor\" href=\"#递归翻页爬取\"></a> 递归翻页爬取</h1>\n<p>目前我们编写的爬虫是从给定的链接中进行爬取的：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&quot;https://quotes.toscrape.com/page/1/&quot;</span>,</span><br><span class=\"line\"><span class=\"string\">&quot;https://quotes.toscrape.com/page/2/&quot;</span></span><br></pre></td></tr></table></figure>\n<p>我们可不可以只给出网站首页，然后爬虫自动解析出下一页的链接持续爬取数据呢？</p>\n<p>我们可以先定位「下一页」的链接在哪里：</p>\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">nav</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;<span class=\"name\">ul</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;pager&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;<span class=\"name\">li</span> <span class=\"attr\">class</span>=<span class=\"string\">&quot;next&quot;</span>&gt;</span></span><br><span class=\"line\">\t\t\t<span class=\"tag\">&lt;<span class=\"name\">a</span> <span class=\"attr\">href</span>=<span class=\"string\">&quot;/page/2/&quot;</span>&gt;</span>Next <span class=\"tag\">&lt;<span class=\"name\">span</span> <span class=\"attr\">aria-hidden</span>=<span class=\"string\">&quot;true&quot;</span>&gt;</span>→<span class=\"tag\">&lt;/<span class=\"name\">span</span>&gt;</span><span class=\"tag\">&lt;/<span class=\"name\">a</span>&gt;</span></span><br><span class=\"line\">\t\t<span class=\"tag\">&lt;/<span class=\"name\">li</span>&gt;</span></span><br><span class=\"line\">\t<span class=\"tag\">&lt;/<span class=\"name\">ul</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">nav</span>&gt;</span></span><br></pre></td></tr></table></figure>\n<p>使用 Scrapy shell：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scrapy shell <span class=\"string\">&#x27;https://quotes.toscrape.com&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定位链接</span></span><br><span class=\"line\">In [<span class=\"number\">1</span>]: response.css(<span class=\"string\">&#x27;li.next a&#x27;</span>).get()</span><br><span class=\"line\">Out[<span class=\"number\">1</span>]: <span class=\"string\">&#x27;&lt;a href=&quot;/page/2/&quot;&gt;Next &lt;span aria-hidden=&quot;true&quot;&gt;→&lt;/span&gt;&lt;/a&gt;&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用CSS的扩展，::attr(href)提取文字</span></span><br><span class=\"line\">In [<span class=\"number\">2</span>]: response.css(<span class=\"string\">&quot;li.next a::attr(href)&quot;</span>).get()</span><br><span class=\"line\">Out[<span class=\"number\">2</span>]: <span class=\"string\">&#x27;/page/2/&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 我们也可以使用attrib方法</span></span><br><span class=\"line\">In [<span class=\"number\">3</span>]: response.css(<span class=\"string\">&quot;li.next a&quot;</span>).attrib[<span class=\"string\">&quot;href&quot;</span>]</span><br><span class=\"line\">Out[<span class=\"number\">3</span>]: <span class=\"string\">&#x27;/page/2/&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>因此，我们的爬虫可以这样写：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> scrapy</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">QuotesSpider</span>(scrapy.Spider):</span><br><span class=\"line\">    name = <span class=\"string\">&quot;quotes&quot;</span></span><br><span class=\"line\">    start_urls = [</span><br><span class=\"line\">        <span class=\"string\">&quot;https://quotes.toscrape.com&quot;</span>,</span><br><span class=\"line\">    ]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">parse</span>(<span class=\"params\">self, response</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> quote <span class=\"keyword\">in</span> response.css(<span class=\"string\">&quot;div.quote&quot;</span>):</span><br><span class=\"line\">            <span class=\"keyword\">yield</span> &#123;</span><br><span class=\"line\">                <span class=\"string\">&quot;text&quot;</span>: quote.css(<span class=\"string\">&quot;span.text::text&quot;</span>).get(),</span><br><span class=\"line\">                <span class=\"string\">&quot;author&quot;</span>: quote.css(<span class=\"string\">&quot;small.author::text&quot;</span>).get(),</span><br><span class=\"line\">                <span class=\"string\">&quot;tags&quot;</span>: quote.css(<span class=\"string\">&quot;div.tags a.tag::text&quot;</span>).getall(),</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">\t\t<span class=\"comment\"># 当第一个页面遍历完毕后，尝试获取第二个页面的链接</span></span><br><span class=\"line\">        next_page = response.css(<span class=\"string\">&quot;li.next a::attr(href)&quot;</span>).get()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> next_page <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            next_page = response.urljoin(next_page)</span><br><span class=\"line\">            <span class=\"comment\"># 新页面的回调函数依然是parse()</span></span><br><span class=\"line\">            <span class=\"keyword\">yield</span> scrapy.Request(next_page, callback=<span class=\"variable language_\">self</span>.parse)</span><br><span class=\"line\">            <span class=\"comment\"># 这里我们yeild了一个scrapy.Request类，Scrapy会调度这个请求，并为这个请求安排回调方法（这里指的是self.parse）</span></span><br></pre></td></tr></table></figure>\n<p>关于 <a href=\"https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Response.urljoin\">urljoin()</a> 的测试：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">web1 = <span class=\"string\">&quot;https://quotes.toscrape.com&quot;</span></span><br><span class=\"line\">web2 = <span class=\"string\">&quot;https://quotes.toscrape.com/page/1/&quot;</span></span><br><span class=\"line\">web3 = <span class=\"string\">&quot;https://quotes.toscrape.com/a/b/c/d/&quot;</span></span><br><span class=\"line\">href = <span class=\"string\">&quot;/page/2/&quot;</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(urllib.parse.urljoin(web1, href))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(urllib.parse.urljoin(web2, href))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(urllib.parse.urljoin(web3, href))</span><br><span class=\"line\"><span class=\"comment\"># 结果都是https://quotes.toscrape.com/page/2/</span></span><br></pre></td></tr></table></figure>\n<p>我们还可以对上面的代码进行简化。我们将最后四行代码（从 next_page 开始）简化为：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 方法 1</span></span><br><span class=\"line\"><span class=\"comment\"># follow 省去了对urljoin的调用，并直接返回Request的实例</span></span><br><span class=\"line\">next_page = response.css(<span class=\"string\">&quot;li.next a::attr(href)&quot;</span>).get()</span><br><span class=\"line\"><span class=\"keyword\">if</span> next_page <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">\t<span class=\"keyword\">yield</span> response.follow(next_page, callback=<span class=\"variable language_\">self</span>.parse)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 方法 2</span></span><br><span class=\"line\"><span class=\"comment\"># 直接向follow传入css选择器</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> href <span class=\"keyword\">in</span> response.css(<span class=\"string\">&quot;ul.pager a::attr(href)&quot;</span>):</span><br><span class=\"line\">    <span class=\"keyword\">yield</span> response.follow(href, callback=<span class=\"variable language_\">self</span>.parse)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 方法 3</span></span><br><span class=\"line\"><span class=\"comment\"># 对于&lt;a&gt;标签，follow可以直接提取其href属性</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> a <span class=\"keyword\">in</span> response.css(<span class=\"string\">&quot;ul.pager a&quot;</span>):</span><br><span class=\"line\">    <span class=\"keyword\">yield</span> response.follow(a, callback=<span class=\"variable language_\">self</span>.parse)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 方法 4 </span></span><br><span class=\"line\"><span class=\"comment\"># 我们可以直接使用follow_all把选择器全部传入，省去for</span></span><br><span class=\"line\">anchors = response.css(<span class=\"string\">&quot;ul.pager a&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">yield</span> <span class=\"keyword\">from</span> response.follow_all(anchors, callback=<span class=\"variable language_\">self</span>.parse)</span><br></pre></td></tr></table></figure>\n<blockquote>\n<p>有一些网站不存在翻页，而是采用无限滚动的方法实现更多内容的浏览（比如这个网站：<a href=\"https://quotes.toscrape.com/scroll\">Quotes to Scrape</a>）。在部分简单的情况下，爬取方法可以参考 <a class=\"uuanqin-bilink\" target=\"_blank\" href=\"/p/dfbe1dad/\"><span class=\"yukari\">站内文章</span>Scrapy-充分利用浏览器中的开发者工具</a> 这篇文章。</p>\n</blockquote>\n<p>这里附一个 XPath 版本的爬虫：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># -*- coding: utf-8 -*-</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> scrapy</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ToScrapeSpiderXPath</span>(scrapy.Spider):</span><br><span class=\"line\">    name = <span class=\"string\">&#x27;toscrape-xpath&#x27;</span></span><br><span class=\"line\">    start_urls = [</span><br><span class=\"line\">        <span class=\"string\">&#x27;http://quotes.toscrape.com/&#x27;</span>,</span><br><span class=\"line\">    ]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">parse</span>(<span class=\"params\">self, response</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> quote <span class=\"keyword\">in</span> response.xpath(<span class=\"string\">&#x27;//div[@class=&quot;quote&quot;]&#x27;</span>):</span><br><span class=\"line\">            <span class=\"keyword\">yield</span> &#123;</span><br><span class=\"line\">                <span class=\"string\">&#x27;text&#x27;</span>: quote.xpath(<span class=\"string\">&#x27;./span[@class=&quot;text&quot;]/text()&#x27;</span>).extract_first(),</span><br><span class=\"line\">                <span class=\"string\">&#x27;author&#x27;</span>: quote.xpath(<span class=\"string\">&#x27;.//small[@class=&quot;author&quot;]/text()&#x27;</span>).extract_first(),</span><br><span class=\"line\">                <span class=\"string\">&#x27;tags&#x27;</span>: quote.xpath(<span class=\"string\">&#x27;.//div[@class=&quot;tags&quot;]/a[@class=&quot;tag&quot;]/text()&#x27;</span>).extract()</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        next_page_url = response.xpath(<span class=\"string\">&#x27;//li[@class=&quot;next&quot;]/a/@href&#x27;</span>).extract_first()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> next_page_url <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">yield</span> scrapy.Request(response.urljoin(next_page_url))</span><br></pre></td></tr></table></figure>\n<p>一个简单的递归爬取名人名言的小爬虫到这里就结束了。继续阅读将学习更多内容。</p>\n<h1 id=\"案例练习爬取作者信息\"><a class=\"markdownIt-Anchor\" href=\"#案例练习爬取作者信息\"></a> 【案例练习】爬取作者信息</h1>\n<p>以下这个例子将爬取作者信息。我们可以新建一个爬虫 <code>author_spider.py</code>：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> scrapy</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">AuthorSpider</span>(scrapy.Spider):</span><br><span class=\"line\">    name = <span class=\"string\">&quot;author&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    start_urls = [<span class=\"string\">&quot;https://quotes.toscrape.com/&quot;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">parse</span>(<span class=\"params\">self, response</span>):</span><br><span class=\"line\">        author_page_links = response.css(<span class=\"string\">&quot;.author + a&quot;</span>) <span class=\"comment\"># 相邻兄弟选择器 +</span></span><br><span class=\"line\">        <span class=\"keyword\">yield</span> <span class=\"keyword\">from</span> response.follow_all(author_page_links, <span class=\"variable language_\">self</span>.parse_author)</span><br><span class=\"line\"></span><br><span class=\"line\">        pagination_links = response.css(<span class=\"string\">&quot;li.next a&quot;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">yield</span> <span class=\"keyword\">from</span> response.follow_all(pagination_links, <span class=\"variable language_\">self</span>.parse)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">parse_author</span>(<span class=\"params\">self, response</span>):</span><br><span class=\"line\">        <span class=\"keyword\">def</span> <span class=\"title function_\">extract_with_css</span>(<span class=\"params\">query</span>):</span><br><span class=\"line\">            <span class=\"keyword\">return</span> response.css(query).get(default=<span class=\"string\">&quot;&quot;</span>).strip()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">yield</span> &#123;</span><br><span class=\"line\">            <span class=\"string\">&quot;name&quot;</span>: extract_with_css(<span class=\"string\">&quot;h3.author-title::text&quot;</span>),</span><br><span class=\"line\">            <span class=\"string\">&quot;birthdate&quot;</span>: extract_with_css(<span class=\"string\">&quot;.author-born-date::text&quot;</span>),</span><br><span class=\"line\">            <span class=\"string\">&quot;bio&quot;</span>: extract_with_css(<span class=\"string\">&quot;.author-description::text&quot;</span>),</span><br><span class=\"line\">        &#125;</span><br></pre></td></tr></table></figure>\n<p>执行命令：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scrapy crawl author -O author.jsonlines</span><br></pre></td></tr></table></figure>\n<p>值得一提的是，尽管一些名言都指向同一个作者页面，Scrapy 会检查这些重复链接并避免多次访问服务器。</p>\n<h1 id=\"进阶小知识使用爬虫参数\"><a class=\"markdownIt-Anchor\" href=\"#进阶小知识使用爬虫参数\"></a> 【进阶小知识】使用爬虫参数</h1>\n<p>我们可以使用 <code>-a</code> 选项向爬虫提供参数。这个参数会直接传入爬虫的 <code>__init__</code> 方法，使之成为爬虫的属性。比如在这个例子中：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> scrapy</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">QuotesSpider</span>(scrapy.Spider):</span><br><span class=\"line\">    name = <span class=\"string\">&quot;quotes&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">start_requests</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        url = <span class=\"string\">&quot;https://quotes.toscrape.com/&quot;</span></span><br><span class=\"line\">\t\ttag = <span class=\"built_in\">getattr</span>(<span class=\"variable language_\">self</span>, <span class=\"string\">&quot;tag&quot;</span>, <span class=\"literal\">None</span>) <span class=\"comment\"># 关键 </span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> tag <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            url = url + <span class=\"string\">&quot;tag/&quot;</span> + tag</span><br><span class=\"line\">        <span class=\"keyword\">yield</span> scrapy.Request(url, <span class=\"variable language_\">self</span>.parse)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">parse</span>(<span class=\"params\">self, response</span>):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> quote <span class=\"keyword\">in</span> response.css(<span class=\"string\">&quot;div.quote&quot;</span>):</span><br><span class=\"line\">            <span class=\"keyword\">yield</span> &#123;</span><br><span class=\"line\">                <span class=\"string\">&quot;text&quot;</span>: quote.css(<span class=\"string\">&quot;span.text::text&quot;</span>).get(),</span><br><span class=\"line\">                <span class=\"string\">&quot;author&quot;</span>: quote.css(<span class=\"string\">&quot;small.author::text&quot;</span>).get(),</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        next_page = response.css(<span class=\"string\">&quot;li.next a::attr(href)&quot;</span>).get()</span><br><span class=\"line\">        <span class=\"keyword\">if</span> next_page <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">            <span class=\"keyword\">yield</span> response.follow(next_page, <span class=\"variable language_\">self</span>.parse)</span><br></pre></td></tr></table></figure>\n<p>执行：</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scrapy crawl quotes -O quotes-humor.json -a tag=humor</span><br></pre></td></tr></table></figure>\n<p>爬虫只会浏览 humor 标签的网页，即 <a href=\"https://quotes.toscrape.com/tag/humor\">https://quotes.toscrape.com/tag/humor</a> 。</p>\n<h1 id=\"本文参考\"><a class=\"markdownIt-Anchor\" href=\"#本文参考\"></a> 本文参考</h1>\n<ul>\n<li><a href=\"https://docs.scrapy.org/en/latest/intro/tutorial.html\">Scrapy Tutorial — Scrapy 2.11.1 documentation</a></li>\n<li><a href=\"https://github.com/scrapy/quotesbot\">scrapy/quotesbot: This is a sample Scrapy project for educational purposes (github.com)</a></li>\n<li><a href=\"https://segmentfault.com/blog/rui0908\">Scrapy详解 - SegmentFault 思否</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/144350043\">Python最火爬虫框架Scrapy入门与实践 - 知乎 (zhihu.com)</a></li>\n<li><a href=\"https://blog.csdn.net/cainiao_python/article/details/119224134\">从原理到实战，一份详实的 Scrapy 爬虫教程-CSDN博客</a></li>\n<li><a href=\"https://blog.csdn.net/ck784101777/article/details/104468780\">Scrapy爬虫框架，入门案例（非常详细）_scrapy爬虫案例python-CSDN博客</a></li>\n<li><a href=\"https://www.zhihu.com/question/21414417\">scrapy 抓取的中文结果乱码，请问如何解决？ - 知乎 (zhihu.com)</a></li>\n</ul>\n","raw":"---\ntitle: Scrapy-入门篇\ntags:\n  - scrapy\n  - python\n  - css\n  - XPath\ncover: 'https://cdn.gallery.uuanqin.top/img/20240220045858.webp'\ndescription: 创建第一个 Scrapy 爬虫\nabbrlink: cf3e4a7b\ncategories:\n  - 核心协同\ndate: 2024-02-20 00:54:08\ntop_img:\n---\n\nScrapy (/ˈskreɪpaɪ/) 是一个爬取网页并提取结构化数据的应用框架，使用 Python 编写。官方文档：[Scrapy at a glance — Scrapy 2.11.1 documentation](https://docs.scrapy.org/en/latest/intro/overview.html)。网上有许多教程，爬虫做是做出来了，但其中的原理可能讲得还不够清晰。我认为阅读官方文档将更有助于理解。\n\n文章内容主要基于官方文档第一章 FIRST STEPS 进行补充。本文将通过爬取英文名人名言网站 https://quotes.toscrape.com 的例子初步理解 Scrapy，学会自己编写一个简单的爬虫。\n\n> 框架：是一个半成品软件，是一套可重用的、通用的、软件基础代码模型。基于框架进行开发，更加快捷、更加高效。\n# 安装\n\n可以使用 pip 进行安装：\n\n```sh\npip install Scrapy\n```\n\n推荐单独为 Scrapy 创建一个 Python 虚拟环境（venv），不管是在哪个平台，以避免和系统中的其它包产生冲突。\n\n# 创建 Scrapy 项目\n\n在你想要创建 Scrapy 项目的位置执行以下命令：\n\n```sh\nscrapy startproject tutorial # tutorial 改为你想要的项目名字\n```\n\n成功效果如图：\n\n![image.png](https://cdn.gallery.uuanqin.top/img/20240220012641.webp)\n\n这时将会多出一个名为 tutorial，其内容如下：\n\n```tree\n.\n│   scrapy.cfg         # 部署参数文件\n└───tutorial           # 项目的Python模块，你将从这里导入你的代码\n    │   items.py       # 项目中定义 items 的文件\n    │   middlewares.py # 项目middlewares文件\n    │   pipelines.py   # 项目pipelines文件\n    │   settings.py    # 项目设置文件\n    │   __init__.py\n    └───spiders        # 存放爬虫的文件夹 \n            __init__.py \n```\n\n我们需要定义一个类（这个类必须是 `scrapy.Spider` 的子类），Scrapy 使用这个类的信息去爬取网站。这个类定义了：\n\n- 爬取的第一个网站\n- 【可选】如何继续生成下一个需要爬取的网页\n- 【可选】如何解析下载下来的页面并提取信息\n\n我们可以在文件夹 `tutorial/spiders` 下创建我们第一个爬虫，文件名为 `quotes_spider.py`：\n\n```python\nfrom pathlib import Path\n\nimport scrapy\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\" # 定义标识这个爬虫的名字，必须在整个项目中唯一\n\n    def start_requests(self):\n\t    \"\"\"\n\t    定义爬虫从哪里开始爬取。\n\t    必须返回一个Requests类的可迭代对象（即一个request的列表或者写一个生成器函数，本示例为后者）。后续的请求将从这些初始请求依次生成。\n\t    \"\"\"\n        urls = [\n            \"https://quotes.toscrape.com/page/1/\",\n            \"https://quotes.toscrape.com/page/2/\",\n        ]\n        for url in urls:\n            yield scrapy.Request(url=url, callback=self.parse)\n\n    def parse(self, response):\n\t    \"\"\"\n\t    用于处理每一个request对应的response的方法。\n\t    参数response是TextResponse的一个实例，它包含下载下来的页面的内容以及有用的处理函数。\n\t    parse()通常用于解析response，提取其中的数据形成字典（dict），然后提取下一个将要被爬取的URL链接并从中创建新的Request请求。\n\t    \"\"\"\n        page = response.url.split(\"/\")[-2]\n        filename = f\"quotes-{page}.html\"\n        Path(filename).write_bytes(response.body)\n        self.log(f\"Saved file {filename}\")\n```\n\n回到项目的顶层目录，执行以下命令运行我们刚刚创建的爬虫：\n\n```sh\nscrapy crawl quotes\n```\n\n![image.png](https://cdn.gallery.uuanqin.top/img/20240220015148.webp)\n\n项目下多出以下两个文件，这两个文件就是两个网站的 HTML 数据。我们将在下一小节实现对数据的提取。\n\n![image.png](https://cdn.gallery.uuanqin.top/img/20240220015246.webp)\n\n让我们看看执行这个命令发生了什么？\n\n1. `start_requests()` 方法返回 `scrapy.Request` 类供 Scrapy 进行调度\n2. 收到每一个 Request 的回复（response）后，Scrapy 实例化 `Response`，并调用对应 Request 的回调函数（在这个例子中回调函数为 `parse()`），将 `Response` 作为回调函数中的参数。\n\n上面的代码我们还可以进行简化：\n\n```python\nfrom pathlib import Path\n\nimport scrapy\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n    # 我们可以直接定义全局变量start_urls，不用编写start_requests()函数\n    start_urls = [\n        \"https://quotes.toscrape.com/page/1/\",\n        \"https://quotes.toscrape.com/page/2/\",\n    ]\n\t# 虽然这里我们没有指明每一个request产生的response如何处理，但Scrapy默认parse()就是它们的回调函数。\n    def parse(self, response):\n        page = response.url.split(\"/\")[-2]\n        filename = f\"quotes-{page}.html\"\n        Path(filename).write_bytes(response.body)\n```\n\n# 提取数据的方法\n\n> 如果你掌握浏览器开发者工具的使用，我们可以很轻松的完成这件事情（详看 [[Scrapy-充分利用浏览器中的开发者工具]]）。\n\n## 使用 CSS 选择器\n\n按照官方文档中的教程，我们可以使用一个工具 Scrapy shell 对爬取的页面进行分析从而选择我们需要提取的数据。\n\n在终端执行命令，我们会进入一个交互程序：\n\n```sh\nscrapy shell 'https://quotes.toscrape.com/page/1/' # windows中可以使用双引号\n# 注意链接使用引号括住，否则在链接包含参数（即链接中存在符号`&`）的情况下命令会不起作用。\n```\n\n> 输入 `quit()` 可退出 Scrapy shell\n\n我们可以 CSS 选择器选择 response 的元素。比如：`response.css(\"title\")`\n\n![image.png](https://cdn.gallery.uuanqin.top/img/20240220021614.webp)\n\n上图显示，CSS 选择器查询结果为 `Selector` 列表，`SelectorList`。这些选择器允许你进行更深入地查询或提取数据。\n\n你可以继续进行交互：\n\n```python\nIn [1]: response.css(\"title\")\nOut[1]: [<Selector xpath='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]\n\n# 加上::text表明我们只选择标签内的文本。getall()方法将返回一个列表，包含所有可能的结果。\nIn [2]: response.css(\"title::text\").getall()\nOut[2]: ['Quotes to Scrape']\n\n# 不加上的话我们将得到包含标签在内的内容\nIn [3]: response.css(\"title\").getall()\nOut[3]: ['<title>Quotes to Scrape</title>']\n\n# get() 直接返回第一个结果。若选择器无结果返回None。\nIn [4]: response.css(\"title::text\").get()\nOut[4]: 'Quotes to Scrape'\n\n# 除此之外还可以这样写。但是如果css没有结果将报错 IndexError: list index out of range\nIn [5]: response.css(\"title::text\")[0].get()\nOut[5]: 'Quotes to Scrape'\n\n# 我们还可以使用正则表达式\nIn [6]: response.css(\"title::text\").re(r\"(\\w+) to (\\w+)\")\nOut[6]: ['Quotes', 'Scrape']\n```\n\n如果你想直接打开爬取到的页面，输入以下命令后页面将从浏览器中被打开方便你使用浏览器自带的调试工具找出正确的 CSS 选择器。\n\n```python\nIn [7]: view(response)\nOut[7]: True\n```\n\n## 使用 XPath\n\nScrapy 支持 XPath 表达式。\n\n```python\nIn [8]: response.xpath(\"//title\")\nOut[8]: [<Selector xpath='//title' data='<title>Quotes to Scrape</title>'>]\n\nIn [9]: response.xpath(\"//title/text()\").get()\nOut[9]: 'Quotes to Scrape'\n```\n\n在 Scrapy 中，推荐使用 XPath 表达。\n\n> 实际上，Scrapy 处理 CSS 选择器最终还是会转换成 XPath 表达。你可以在 shell 中发现这个过程。\n\n\n## 关于提取的方法\n\n不管是 css 选择器还是 XPath 表达，都可以使用以下方法提取我们需要的内容：\n\n|方法 |描述 |\n|---|---|\n|`extract()` |返回的是符合要求的所有的数据，存在一个列表里。|\n|`extract_first()` |返回的 hrefs 列表里的第一个数据。|\n|`get()` |和 extract_first() 方法返回的是一样的，都是列表里的第一个数据。|\n|`getall()` |和 extract() 方法一样，返回的都是符合要求的所有的数据，存在一个列表里。|\n\n注意：\n\n- get() 、getall() 方法是新的方法，取不到就 raise 一个错误。\n- extract() 、extract_first() 方法是旧的方法，取不到就返回 None。\n\n> 本文按照官方文档示例继续使用旧的 get、getall 方法\n\n# 爬虫中编写提取数据的逻辑\n\n在目标网站 https://quotes.toscrape.com 中，每一句名人名言的 HTML 基本结构如下（以某一句名言为例）：\n\n```html\n<div class=\"quote\" itemscope=\"\" itemtype=\"http://schema.org/CreativeWork\">\n\t<span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>\n\t<span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n\t<a href=\"/author/Albert-Einstein\">(about)</a>\n\t</span>\n\t<div class=\"tags\">\n\t\tTags:\n\t\t<meta class=\"keywords\" itemprop=\"keywords\" content=\"change,deep-thoughts,thinking,world\"> \n\t\t<a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n\t\t<a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n\t\t<a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n\t\t<a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n\t</div>\n</div>\n```\n\n我们可以在 Scrapy shell 中一步步找出我们想要的数据：\n\n```python\nscrapy shell 'https://quotes.toscrape.com'\n\nIn [1]: response.css(\"div.quote\")\nOut[1]:\n[<Selector xpath=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n......\n\n# 定位名言\nIn [2]: quote = response.css(\"div.quote\")[0]\n\n# 定位并验证名言文本\nIn [3]: text = quote.css(\"span.text::text\").get()\nIn [4]: text\nOut[4]: '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'\n\n# 定位并验证名言作者\nIn [5]: author = quote.css(\"small.author::text\").get()\nIn [6]: author\nOut[6]: 'Albert Einstein'\n\n# 定位并验证名言标签\nIn [7]: tags = quote.css(\"div.tags a.tag::text\").getall()\nIn [8]: tags\nOut[8]: ['change', 'deep-thoughts', 'thinking', 'world']\n\n# 尝试编写一段这样的逻辑，验证一下我们上面定位是否都准确\nIn [13]: for quote in response.css(\"div.quote\"):\n    ...:     text = quote.css(\"span.text::text\").get()\n    ...:     author = quote.css(\"small.author::text\").get()\n    ...:     tags = quote.css(\"div.tags a.tag::text\").getall()\n    ...:     print(dict(text=text, author=author, tags=tags))\n    ...:\n{'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}\n{'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}\n......\n```\n\n验证无误后，我们将这段代码写入爬虫的 parse() 中：\n\n```python\ndef parse(self, response):\n\tfor quote in response.css(\"div.quote\"):\n\t\tyield {\n\t\t\t\"text\": quote.css(\"span.text::text\").get(),\n\t\t\t\"author\": quote.css(\"small.author::text\").get(),\n\t\t\t\"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n\t\t}\n```\n\n终端运行以下命令：\n\n```sh \nscrapy crawl quotes # 还记得吗？quotes是我们编写的爬虫的名字\n```\n\n你可以在终端输出的日志中读到爬取的内容：\n\n![image.png](https://cdn.gallery.uuanqin.top/img/20240220033017.webp)\n\n# 存储抓取的数据\n\n最简单的方式为 Feed exports：\n\n```sh\nscrapy crawl quotes -O quotes.json\n# quotes 为爬虫的名字\n# -O （大写字母O) 表示将覆写已经存在的同名文件；-o 则是将内容附加在已经存在的文件\n# quotes.json为输出的文件名\n```\n\n我们可以指定输出文件的格式。上例为 JSON，但是如果我们追加输出到已经存在的文件时会导致生成无效的 json 文件。为此我们可以使用 JSON Lines 格式：\n\n```sh\nscrapy crawl quotes -o quotes.jsonlines # 亲测jsonl会报错\n```\n\n![image.png](https://cdn.gallery.uuanqin.top/img/20240220033829.webp)\n\n> [!hint] 使用 Scrapy 爬取中文数据时的注意事项\n> 如果爬取数据为中文，存储到文件可能是一堆 unicode 编码。我们可以在 `setting.py` 中增加：`FEED_EXPORT_ENCODING = 'utf-8'` 即可\n# 递归翻页爬取\n\n目前我们编写的爬虫是从给定的链接中进行爬取的：\n\n```python\n\"https://quotes.toscrape.com/page/1/\",\n\"https://quotes.toscrape.com/page/2/\"\n```\n\n我们可不可以只给出网站首页，然后爬虫自动解析出下一页的链接持续爬取数据呢？\n\n我们可以先定位「下一页」的链接在哪里：\n\n```html\n<nav>\n\t<ul class=\"pager\">\n\t\t<li class=\"next\">\n\t\t\t<a href=\"/page/2/\">Next <span aria-hidden=\"true\">→</span></a>\n\t\t</li>\n\t</ul>\n</nav>\n```\n\n使用 Scrapy shell：\n\n```python\nscrapy shell 'https://quotes.toscrape.com'\n\n# 定位链接\nIn [1]: response.css('li.next a').get()\nOut[1]: '<a href=\"/page/2/\">Next <span aria-hidden=\"true\">→</span></a>'\n\n# 使用CSS的扩展，::attr(href)提取文字\nIn [2]: response.css(\"li.next a::attr(href)\").get()\nOut[2]: '/page/2/'\n\n# 我们也可以使用attrib方法\nIn [3]: response.css(\"li.next a\").attrib[\"href\"]\nOut[3]: '/page/2/'\n```\n\n因此，我们的爬虫可以这样写：\n\n```python\nimport scrapy\n\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n    start_urls = [\n        \"https://quotes.toscrape.com\",\n    ]\n\n    def parse(self, response):\n        for quote in response.css(\"div.quote\"):\n            yield {\n                \"text\": quote.css(\"span.text::text\").get(),\n                \"author\": quote.css(\"small.author::text\").get(),\n                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n            }\n\t\t# 当第一个页面遍历完毕后，尝试获取第二个页面的链接\n        next_page = response.css(\"li.next a::attr(href)\").get()\n        if next_page is not None:\n            next_page = response.urljoin(next_page)\n            # 新页面的回调函数依然是parse()\n            yield scrapy.Request(next_page, callback=self.parse)\n            # 这里我们yeild了一个scrapy.Request类，Scrapy会调度这个请求，并为这个请求安排回调方法（这里指的是self.parse）\n```\n\n关于 [urljoin()](https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Response.urljoin) 的测试：\n\n```python\nweb1 = \"https://quotes.toscrape.com\"\nweb2 = \"https://quotes.toscrape.com/page/1/\"\nweb3 = \"https://quotes.toscrape.com/a/b/c/d/\"\nhref = \"/page/2/\"\nprint(urllib.parse.urljoin(web1, href))\nprint(urllib.parse.urljoin(web2, href))\nprint(urllib.parse.urljoin(web3, href))\n# 结果都是https://quotes.toscrape.com/page/2/\n```\n\n我们还可以对上面的代码进行简化。我们将最后四行代码（从 next_page 开始）简化为：\n\n```python\n# 方法 1\n# follow 省去了对urljoin的调用，并直接返回Request的实例\nnext_page = response.css(\"li.next a::attr(href)\").get()\nif next_page is not None:\n\tyield response.follow(next_page, callback=self.parse)\n\n# 方法 2\n# 直接向follow传入css选择器\nfor href in response.css(\"ul.pager a::attr(href)\"):\n    yield response.follow(href, callback=self.parse)\n\n# 方法 3\n# 对于<a>标签，follow可以直接提取其href属性\nfor a in response.css(\"ul.pager a\"):\n    yield response.follow(a, callback=self.parse)\n\n# 方法 4 \n# 我们可以直接使用follow_all把选择器全部传入，省去for\nanchors = response.css(\"ul.pager a\")\nyield from response.follow_all(anchors, callback=self.parse)\n```\n\n> 有一些网站不存在翻页，而是采用无限滚动的方法实现更多内容的浏览（比如这个网站：[Quotes to Scrape](https://quotes.toscrape.com/scroll)）。在部分简单的情况下，爬取方法可以参考 [[Scrapy-充分利用浏览器中的开发者工具]] 这篇文章。\n\n这里附一个 XPath 版本的爬虫：\n\n```python\n# -*- coding: utf-8 -*-\nimport scrapy\n\nclass ToScrapeSpiderXPath(scrapy.Spider):\n    name = 'toscrape-xpath'\n    start_urls = [\n        'http://quotes.toscrape.com/',\n    ]\n\n    def parse(self, response):\n        for quote in response.xpath('//div[@class=\"quote\"]'):\n            yield {\n                'text': quote.xpath('./span[@class=\"text\"]/text()').extract_first(),\n                'author': quote.xpath('.//small[@class=\"author\"]/text()').extract_first(),\n                'tags': quote.xpath('.//div[@class=\"tags\"]/a[@class=\"tag\"]/text()').extract()\n            }\n\n        next_page_url = response.xpath('//li[@class=\"next\"]/a/@href').extract_first()\n        if next_page_url is not None:\n            yield scrapy.Request(response.urljoin(next_page_url))\n```\n\n一个简单的递归爬取名人名言的小爬虫到这里就结束了。继续阅读将学习更多内容。\n\n# 【案例练习】爬取作者信息\n\n以下这个例子将爬取作者信息。我们可以新建一个爬虫 `author_spider.py`：\n\n```python\nimport scrapy\n\nclass AuthorSpider(scrapy.Spider):\n    name = \"author\"\n\n    start_urls = [\"https://quotes.toscrape.com/\"]\n\n    def parse(self, response):\n        author_page_links = response.css(\".author + a\") # 相邻兄弟选择器 +\n        yield from response.follow_all(author_page_links, self.parse_author)\n\n        pagination_links = response.css(\"li.next a\")\n        yield from response.follow_all(pagination_links, self.parse)\n\n    def parse_author(self, response):\n        def extract_with_css(query):\n            return response.css(query).get(default=\"\").strip()\n\n        yield {\n            \"name\": extract_with_css(\"h3.author-title::text\"),\n            \"birthdate\": extract_with_css(\".author-born-date::text\"),\n            \"bio\": extract_with_css(\".author-description::text\"),\n        }\n```\n\n执行命令：\n\n```sh\nscrapy crawl author -O author.jsonlines\n```\n\n值得一提的是，尽管一些名言都指向同一个作者页面，Scrapy 会检查这些重复链接并避免多次访问服务器。\n\n# 【进阶小知识】使用爬虫参数\n\n我们可以使用 `-a` 选项向爬虫提供参数。这个参数会直接传入爬虫的 `__init__` 方法，使之成为爬虫的属性。比如在这个例子中：\n\n```python\nimport scrapy\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n\n    def start_requests(self):\n        url = \"https://quotes.toscrape.com/\"\n\t\ttag = getattr(self, \"tag\", None) # 关键 \n        if tag is not None:\n            url = url + \"tag/\" + tag\n        yield scrapy.Request(url, self.parse)\n\n    def parse(self, response):\n        for quote in response.css(\"div.quote\"):\n            yield {\n                \"text\": quote.css(\"span.text::text\").get(),\n                \"author\": quote.css(\"small.author::text\").get(),\n            }\n\n        next_page = response.css(\"li.next a::attr(href)\").get()\n        if next_page is not None:\n            yield response.follow(next_page, self.parse)\n```\n\n执行：\n\n```sh\nscrapy crawl quotes -O quotes-humor.json -a tag=humor\n```\n\n爬虫只会浏览 humor 标签的网页，即 https://quotes.toscrape.com/tag/humor 。\n\n# 本文参考\n- [Scrapy Tutorial — Scrapy 2.11.1 documentation](https://docs.scrapy.org/en/latest/intro/tutorial.html)\n- [scrapy/quotesbot: This is a sample Scrapy project for educational purposes (github.com)](https://github.com/scrapy/quotesbot)\n- [Scrapy详解 - SegmentFault 思否](https://segmentfault.com/blog/rui0908)\n- [Python最火爬虫框架Scrapy入门与实践 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/144350043)\n- [从原理到实战，一份详实的 Scrapy 爬虫教程-CSDN博客](https://blog.csdn.net/cainiao_python/article/details/119224134)\n- [Scrapy爬虫框架，入门案例（非常详细）_scrapy爬虫案例python-CSDN博客](https://blog.csdn.net/ck784101777/article/details/104468780)\n- [scrapy 抓取的中文结果乱码，请问如何解决？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/21414417)","categories":[{"name":"核心协同","api":"api/categories/核心协同.json"}],"tags":[{"name":"python","api":"api/tags/python.json"},{"name":"scrapy","api":"api/tags/scrapy.json"},{"name":"css","api":"api/tags/css.json"},{"name":"XPath","api":"api/tags/XPath.json"}]},"api":"api/posts/p/cf3e4a7b.json"}